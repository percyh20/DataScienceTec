{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis de Proyecto Final Conclusion\n",
    "\n",
    "## Jason Solano\n",
    "\n",
    "## Percy Herrera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento ADA\n",
    "\n",
    "En el procesamiento de los datos crudos pudimos estudiar como ciertas columnas necesitaron un tratamiento para convertirlos en datos aptos para el estudio y entrenamiento de nuestros modelos. \n",
    "\n",
    "Conforme fuimos elaborando los pasos del procesamiento pudimos ir viendo como algunas columnas les faltaban datos y debiamos corregir esta situacion y al mismo tiempo analizar cuales no eran aptas para tomar en cuenta para nuestro estudio.\n",
    "\n",
    "Algunas de las columnas de fecha era correcto y esperado que tuvieran datos nulos, por que indicaban el momento en el que fue ejecutada una accion, como el caso de nuestra columna target redeem_ts que indica el momento en el que fue canjeada una oferta de cierre de una deuda.\n",
    "\n",
    "Para estos datos decimidos cambiarlos por valores True y False, dado que nos era mas significativo saber quienes habian hecho uso de la oferta para cerrar sus deudas y cuales no, por lo que procedimos a realizar dicha transformacion y convertir en 0 y 1, al final la idea del modelo era predecir cuales clientes harian uso de la oferta para cancelar sus deudas, por lo que el valor en fecha no nos es significativo.\n",
    "\n",
    "Luego de esto procedimos a balancear los datos y corregir situaciones similares de las otras columnas, concluyendo con un trabajo normal de preprocesamiento de las demas columnas.\n",
    "\n",
    "Dado que teniamos columnas categoricas que considerabamos contenian datos importantes para el estudio de nuestros datos procedimos a analizar varias opciones de modelos y transformacion de estos datos para el correcto uso en el entrenamiento de dichos modelos, utilizamos encoding de estas para usar el dataframe en el entrenamiento de una red neuronal e intentar lograr generalizar con dichos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiendo de modelos\n",
    "\n",
    "El entrenamiento y generalizacion de nuestro modelo de datos contaba con el desafio de ser un dataset de la vida real totalmente crudo obtenido de la base de datos del trabajo, por lo que los datos no vienen para nada facil de categorizar, si no que cuentan con un nivel de preprocesamiento grande y complejo, pero decidimos que seria un reto interesante a enfrentar por que representa el escenario normal del dia a dia de la ciencia de datos.\n",
    "\n",
    "### Red Neuronal MLP\n",
    "\n",
    "Para el entrenamiento de la red neuronal teniamos que transformar nuestras columnas categoricas a datos binarios por medio de tecnicas de one hot encoding o pandas get dummies, para convertir nuestros datos en columnas y poder entrenar, al final nuestro set de datos quedo muy grande por dicha conversion, por lo que el entrenamiento de nuestra red neuronal se complico y empezamos a obtener un overfiting considerable.\n",
    "\n",
    "Utilizamos varias tecnicas vistas en clase para mejorar estos puntajes de precision, logrande mejorar los numeros de testing pero continuo presentando overfiting.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Con la utilizacion de este modelo obtuvimos los primeros analizis de importancia de variables y esto nos dio mucha informacion importante para estudiar cuales variables nos estaban aportando la mayoria de la informacion y poder eliminar las variables que nos estaban agregando ruido al entrenamiento.\n",
    "\n",
    "Con este modelo logramos obtener los mejores resultados y hasta el momento el modelo escogido para presentar como resultados finales del proyecto final\n",
    "\n",
    "### Catboost\n",
    "\n",
    "Con este modelo decidimos estudiar e investigar uno de los modelos ampliamente usado y nuevo dentro de la ciencia de datos para entrenamiento de set de datos con grandes cantidades de variables categoricas, con este modelo y su integracion con varias librerias de visualizacion de datos logramos obtener varios graficos importantes para estudiar nuestro set de datos.\n",
    "\n",
    "Consideramos que el entrenamiento de nuestro set de datos usando este modelo es bastante confiable, hasta el momento no logramos generalizar bien con este modelo, pero nos muestra mucha informacion de calidad de los resultados de nuestros entrenamientos, creemos que vale la pena seguir estudiando este modelo despues de presentado el proyecto final, dado que presenta una variedad de opciones importantes que merecen estudio y analisis posterior para lograr aprovechar todo su potencial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
