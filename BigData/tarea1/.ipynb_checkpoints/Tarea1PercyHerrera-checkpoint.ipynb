{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guia de instalacion Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente guia aprenderemos como instalar Apache Spark en una maquina con sistema operativo Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga de Apache Spark\n",
    "\n",
    "Como primer paso necesitamos bajar la version mas reciente de apache spark\n",
    "\n",
    "[Descargar Spark](http://spark.apache.org/downloads.html)\n",
    "\n",
    "\n",
    "<img src=\"imagen1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando finalize la descarga creamos un folder donde descomprimir nuestro archivo de Spark por ejemplo\n",
    "\n",
    "**C:\\Spark**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga de Java 8\n",
    "\n",
    "Para la ejecucion de Spark como requisito necesitamos una maquina virtual de java corriendo en nuestra computadora\n",
    "por lo que necesitamos instalar Java 8.\n",
    "\n",
    "[Descargar Java 8](https://www.oracle.com/technetwork/java/javase/jre8-downloads-2133155.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagen2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar Winutils\n",
    "\n",
    "Para la correcta ejecucion de Spark necesitamos obtener el paquete de Winutils de Hadoop 2.7, esto es necesario para asegurarnos de que spark funcione correctamente en windows, en el siguiente link se explica el por que la necesidad de este paquete [Windows problems](https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems)\n",
    "\n",
    "Ocupamos descargar el repositorio de [Aqui](https://github.com/steveloughran/winutils)\n",
    "\n",
    "y luego copiamos la carpeta de nuestra version a una ruta conocida por ejemplo \"C:\\Hadoop-2.7.1\"\n",
    "\n",
    "<img src=\"imagen3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables de Entorno\n",
    "\n",
    "En concreto vamos a crear tres variables de entorno…\n",
    "\n",
    "SPARK_HOME. Ruta al directorio donde hemos descomprimido el paquete de Apache Spark.\n",
    "\n",
    "HADOOP_HOME. Apunta al directorio donde hemos copiado la carpeta con la Winutils.\n",
    "\n",
    "JAVA_HOME. Es el directorio donde se ha instalado el JRE de Java 8\n",
    "\n",
    "<img src=\"imagen4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciando Apache Spark\n",
    "\n",
    "Todo lo que necesitamos para arrancar nuestra instalación de Apache Spark se encuentra dentro de la carpeta bin de Apache Spark.\n",
    "\n",
    "Ahora en una terminal nos dirigimos al folder donde tenemos nuestro Spark instalado y ejecutamos el siguiente comando\n",
    "\n",
    "**spark-class org.apache.spark.deploy.master.Master**\n",
    "\n",
    "<img src=\"imagen5.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora lo que nos interesa es esta URL donde nuestro master de spark esta corriendo, lo necesitamos para iniciar un **worker** \n",
    "nuestro cluester de Spark con el siguiente comando\n",
    "\n",
    "**spark-class org.apache.spark.deploy.worker.Worker spark://192.168.2.24:7077**\n",
    "\n",
    "<img src=\"imagen6.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accesando la direccion http://desktop-8tudrnh:8080/ podremos ver el UI de nuestro cluster de Spark\n",
    "\n",
    "<img src=\"imagen7.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar FindSpark\n",
    "\n",
    "Como parte del ambiente que necesitamos preparar para el estudio de Spark necesitamos instalar el siguiente paquete que utilizaremos en nuestros notebooks.\n",
    "\n",
    "Abriremos una terminal de nuestro ambiente virtual en Anaconda y ejecutaremos lo siguiente.\n",
    "\n",
    "**conda install -c conda-forge findspark**\n",
    "\n",
    "<img src=\"imagen8.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar PySpark\n",
    "\n",
    "Para nuestro ambiente de desarrollo y analizis tambien necesitamos del paquete PySpark, el cual instalamos con el siguiente comando.\n",
    "\n",
    "**conda install -c conda-forge pyspark**\n",
    "\n",
    "<img src=\"imagen9.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba\n",
    "\n",
    "Para comprobar que la instalacion fue correcta por favor ejecutar las dos celdas de codigo siguientes que utilizan y levantan un ambiente de trabajo con los paquetes antes instalados.\n",
    "\n",
    "Si no tenemos errores el proceso de instalacion fue satisfactorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('data_processing').getOrCreate()\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\Spark\\spark-3.0.0-preview-bin-hadoop2.7\")\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
